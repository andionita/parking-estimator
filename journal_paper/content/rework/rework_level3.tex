%\documentclass{article}
\documentclass{ws-ijait}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usepackage{tikz-3dplot}
\usepackage{enumitem}
\usepackage{cleveref}
\usepackage[mode=buildnew]{standalone}
\usepackage{mathtools} % for dcases

\newcommand{\cmmnt}[1]{\ignorespaces}

\begin{document}
	\section{Introduction}
	
	\section{Related Work}

	\section{Idea behind the Approach and assumptions}
	The approach presented offers a solution to estimating parking occupancy without the help of sensor data. It is based on the observation that parking is determined by the specificities of city areas. Two residential neighborhoods of similar sizes, perhaps far apart from each other, will have very similar parking occupancies: high during nighttime and low during daytime. This will likely differ significantly from office areas, which tend to have most parking spaces occupied during the day and free during the night. Restaurants or shopping centers may represent another distinct category, where customers park usually during the evenings and on weekends, while in the other times they are not very busy, therefore producing a low parking occupancy.
	
	Looking at a city from above, we can see a pattern: the types of buildings and the time people spend there determines parking behavior. The presented approach will build on this pattern in order to estimate the level of parking occupancy. Specifically, it will use amenity types and time-spent data to complement established machine learning algorithms in order to arrive at parking levels in places where such forecasts cannot be made only with straightforward models.
	
	The approach will \textit{not} infer parking levels solely based on building metadata and busy times. This information is currently not enough and other factors regarding the city would be needed to arrive at a direct result. Some cities have better parking infrastructures while fitting the same number of people in the offices as cities with scant parking facilities. In the former parking will likely be concentrated around the offices, while in the latter the cars will probably be distributed uniformly around a larger area around the offices. To circumvent these inconsistencies between cities, we focus on single cities, where parking infrastructure is likely the same given the type of amenities and their dimension. This could be extended to cities in a region or a whole country, depending on the specifics.
	
	The approach will therefore use the (dis)similarities between city areas, with their respective amenity types and time-spent information, to help infer parking occupancy. Hence, we assume that an estimation model can be transferred from a source city area A to a target city area B without any amendment, provided A and B are perfectly similar according to their parking profile. In contrast, the parking occupancy estimation will very much differ if A and B are dissimilar. Specifically, the approach will not offer a precise result in this case, resorting to an interval express the possible parking level.
	
	\subsection{Motivational example}
	The dimension of the problem we are attempting to solve here is best illustrated with a concrete scenario.
	
	Bob is excited about the interview with a big IT company in his city. He will be driving to the office building located in the one of several office sites in the city. Bob does not like being late, even more so on this occasion and he wants to leave himself enough buffer time before he arrives at the company reception desk. He has no idea about the parking situation on site, however. In this city, he could spend up to half an hour to find a free parking space. Therefore Bob uses a new parking app, that can estimate the parking levels at almost every location; the system does not employ sensors everywhere, instead it works by extending the parking behavior from a site to another depending on their specificity, be it offices, restaurants, shopping or residential. Bob likes the idea and enters his estimated time of arrival at the site and sees that the parking occupancy there will be between 60\% and 80\%. This is good enough for him, he knows that at least 1 out of 5 spaces will be free on average and will likely find a spot in a few minutes. He is suddenly more confident about his punctuality and can now drive more assured to the interview.
		
	\section{Approach}
	The approach, in its generic form, is split into several steps. It begins by acquiring access to data that contains information about parking occupancy for a certain area. To spatially reference the parking data, it is mapped to geographically corresponding OpenStreetMap (OSM) data. The Points-of-Interest (POIs) from the OSM data are spatially clustered so that the individual clusters are of about the same size. A machine learning model is trained on parking data for each cluster. Also, for each cluster, mathematical representations are constructed based on the OSM data. Next, similarity values are computed between pairs of clusters using Cosine Similarity and Earth Mover's Distance. Finally, estimations of parking occupancy are computed by applying the models on areas without parking data with the similarity values factored in.   
	
	\subsection{Overview}
	
	\begin{enumerate}[label=\Roman*]
				
		\item{\textbf{Get access to parking data}}
		
		Finding appropriate data is the first step. Parking occupancy information is usually captured by stationary sensors, mounted on lampposts or in the ground. Sometimes the sensors are installed in cars that drive around, but the data they capture is less reliable, as changes in occupancy are not caught. Imaging sensors are preferred, but acoustic ones are also used.
		
		To have a solid analysis foundation, it is essential to find a well-defined spatial area for which measurements over a continuous period of time have been made. Regular status updates, usually by hour, are preferred, if not as soon as they happen. In case more multiple distinct data sources for the spatial area and time period are available, limiting oneself to the richest data source is recommended, as multiple sources tend to be inconsistent with regard to sensor errors.
		
		%Upon making the data selection, it will be annotated in RDF format so that it contextualized and makes it easy for future references to address it.  
		
		\item{\textbf{Map the parking data to OpenStreetMap layers}}
		
		An essential part is geographically referencing the parking data. OpenStreetMap layers such as points, lines and polygons that include geographical coordinates, street coordinates and building shapes respectively, together with other metadata will be downloaded and mapped to the occupancy information.
		
		\item{\textbf{Cluster the spatially-referenced data into multiple city areas}}
		
		Splitting the data into multiple groups is central to the goal of the approach. Specifically, having city areas without parking data completely separated from the city areas with parking data so that the latter can later serve as estimation basis for the former. 
		
		The splitting in the two groups will be spatially. Including any other property, such as building metadata, in the clustering algorithm would result in incontinuous areas, which would defeat the ultimate purpose of a driver finding a parking space inside a certain radius. Furthermore, the resulting clusters should be of about the same size, as it helps to make inferences later in the process. Averaging the occupancy among the parking spaces inside a cluster, for instance, is less representative for another cluster that has a number of parking spaces of a different order of magnitude.		
		
		\item{\textbf{Build machine learning models for each city area}}
		For each city area cluster a machine learning model will be built. The predictor variables includes date and time, parking lot capacity, and parking price, while the target variable is the parking occupancy. 
		
		Methods used for building the models are Decision Trees, Support Vector Machines, Multilayer Perceptrons, and Boosted Trees.
		
		\item{\textbf{Build mathematical representations for each city area}}
		Apart from the parking information, we can find complementary data on the clustered city areas. 
		
		On the one hand, we have the points of interest, lines and polygon layers that OpenStreetMap offers contain a variety of metadata. The amenities are the most relevant in this case, containing types of buildings, facilities, institutions, offices, opening times, etc. 
		
		On the other hand, services such as Google Maps and FourSquare offer data on the time people typically spend in amenities. Since the time spent information is assumed to reflect parking demand, this data will help us augment the occupancy estimations. Some of the data can be collected through APIs, other is yet to have been made available programatically.  
		
		Equipped with the above pieces of information mathematical representations can be build, such as vectors and density estimation kernels.  		
		
		\item{\textbf{Compute similarity values between any two city areas}}
		The mathematical representations built in the previous step make it possible to define similarity measures between city areas. Cosine similarity can be applied between the vectors constructed on time-spent and amenity data. Likewise, earth mover's distance can be applied on pairs of density estimation kernels constructed previously.
		
		\item{\textbf{Apply models on city areas that do not have parking information}}
		In order to compute the occupancy in clustered city areas with no parking data we need to put together the elements that we built up to now. Basically, the trained machine learning models are applied to the clustered areas without parking data. In the result the similarity measure between the originating model area and the target area is factored in.
		
		In practice, this means that records for the target area will need to be constructed to represent the predictor variable using average values from clusteres with parking data. The result outputted by the model will be extended in form of an interval upon applying the similarity value: the smaller the similarity, the more the interval will be stretched around the original occupancy result. Parking occupancy interval is expressed between 0\% and 100\%.
		
	\end{enumerate}
	
	\subsection{Get access to parking data}
	We consider the following types of data as parking data: \textit{parking occupancy} contains information on the availability of parking spaces; \textit{traffic data} contains information regarding the city traffic, which is relevant for parking; \textit{weather data} contains weather information for the same area as for the parking problem; \textit{event data} contains event information which may have an impact on parking; \textit{parking revenue data} contains economic information on parking, whose relevance may influence parking prediction. \textit{fuel price data} contains prices of fuel in the region for which we build the models.
	Each piece of data is geographically referenced by a \textit{location unit}, e.g., street block, district or city. 
	An overview of the different properties available in the data set is shown in \cref{tab:sfpark_data}.
	
	\begin{table}
		\tbl{An overview of the properties available in the data used from the SFpark project}
		{\begin{tabular}{lp{4cm}lp{4cm}}	
				\toprule
				Parking Occupancy & & Traffic & \\
				%\midrule
				\colrule
				date and time & Recorded usually at full hours or in periodic time intervals & date and time & recorded usually at full hours or in periodic time intervals \\
				parking capacity & The total number of parking spaces at the given location & traffic value & typically expressed as average traffic road occupancy, average vehicle count, median speed, or average speed of the traveling cars \\
				parking price & The price of a ticket at the certain location and the given time in a given currency & & \\
				parking occupancy & Expressed either as rate (subunitary fraction or percent) or in absolute numbers & & \\
				%\midrule
				\colrule
				Events & & Weather & \\
				%\midrule
				\colrule
				event name class & the name of the event and its class (road closure, rise of parking demand) & temperature & may be current temperatures or maximum and minimum values per day \\
				& & 	precipitation & expressing the quantity of rain or snow for the corresponding time interval \\
				%\midrule
				\colrule
				Fuel Price & & Parking revenue & \\
				%\midrule	 
				\colrule
				type of fuel & gasoline, diesel, etc. & payment type & the way the driver opted to pay for parking, e.g., cash, credit card, etc. \\
				
				price per unit & provided as the price per liter or per gallon. & payed amount & the amount in US dollars, Euro or other currency\\
				
				%\bottomrule
				\botrule
		\end{tabular}}
		\begin{tabnote}
			Each of these data types also has the location unit id, as well as the date and time (interval) when it occurred or was measured. 
			In some cases, the location is a somewhat larger or smaller area as the unit. The time information is provided in different granularities (e.g., per minute, per hour, per day, etc. )
		\end{tabnote}
		\label{tab:sfpark_data}
	\end{table}
	
	\subsection{Map parking data to OpenStreetMap layers}
	We complement the parking data by downloading OpenSteetMap\footnote{\url{https://www.openstreetmap.org} The maps used in this article are \textcopyright OpenStreetMap contributors} data corresponding to the location where the parking data belongs to. OSM data is generally available as shapefiles containing the geometry layers: points, polylines, and polygons. We extract the \textit{points of interest} (POIs), which, among multiple attributes, contain the \textit{amenity} attribute indicating the public service, facility, or type of building located at this position as it was annotated by the OSM users (cf. \cref{fig:pois}). The polylines layer contains artifacts mostly in linear form, such as streets or foot paths. Polylines are less interesting for our problem and therefore we will ignore them. The polygons layer contains artifacts of polygon shapes such as buildings, parks, university campuses, etc. Polygon objects may contain an amenity attribute as well, in practice the authors have found it often empty, however. When the attribute is present, it enables us to compute the area of the amenity and make an inference towards the capacity of the building, and by extension, towards its parking demand.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.8\textwidth]{../graphics/cafes_restaurants_banks_larger.png}
		\caption{A map indicating public amenities (cafes, restaurants, banks) found at points of interest in OSM.}
		\label{fig:pois}
	\end{figure}
	
	Furthermore, we collect the vising duration of amenities. This information is offered by Google Places and FourSquare. The latter data is available via an API, however the access not free. We collected data from Google Places corresponding to the parking data's location. An example of the service is withing Google Maps for mobile phones. It displays typical \textit{visiting duration} or \textit{time spent} values and popularity of the place for specific points in time. The average values are based on the users' smart phone GPS sensors (cf. \cref{fig:visit_duration}). To obtain the duration values, we manually extract information from Google Maps. The duration information is aggregated by Google using a crowdsourcing approach. 
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.8\textwidth]{../graphics/google_visit_duration.png}
		\caption{An example of \textit{visiting duration} information found on Google Maps.}
		\label{fig:visit_duration}
	\end{figure}  
	
	In order to combine the parking and OSM data (or city data), both sets of data require a common location unit. The parking location units are provided together alongside the various types of parking data. The city data, on the other hand, references POI geometries, which are points expressed in a particular reference system, which differs from the one of the parking locations. Therefore, after establishing the coordinate systems of both geometries, we define a \textit{merge distance} that matches a parking space to a public amenity. 
	
	The merge distance can be intuitively understood as the radius around a public amenity. It is defined to represent the parking area that is relevant for a particular public amenity, or, more straightforward, the walking distance from the parked car to e.g., the restaurant, the office, the bank, etc. Concrete instances of the merging distance can be found in \cmmnt{\cref{experimental_setup:merging_parking_city_data}} the evaluation section.
	
	\subsection{The clustering process}
	By splitting into city areas, we are making sure that smaller regions lead to more representative parking profiles and therefore parking estimations. 
	
	As we want an exclusively location-based separation, we may employ K-Means, DBSCAN or OPTICS to cluster the city areas. The distance is calculated between $($latitude$,$ longitude$)$ -pairs of location unit coordinates corresponding to one parking unit. There are two clustering processes executed, one for the city area \textit{with} parking data, another one for the city area \textit{without} parking data. The number of clusters chosen in each area is kept proportional to the number of total location units that each city area contains. Since having control over the number of clusters is the goal here, we choose to use K-Means, where we provide the number of expected clusters as input. More details on the concrete value of $k$ and an overview about the clusters can be found in the evaluation section \cmmnt{\cref{experimental_setup:clustering}}.
	
	\subsection{Build estimation models}
	The estimation of parking occupancy is realized using machine learning. We choose to explore this methodology following the solid results machine learning models have delivered for the various smart parking settings investigated in the related work section \cmmnt{\cref{sec:relwork}}. A machine learning model $\mathcal{M}$ will be trained for every cluster \textit{with} parking data.
	
	The training data \textit{features} is composed from the parking data. Specifically, date and time, parking capacity and parking price act as predictor variables, while the occupancy rate is the target variable. When training, all values in a cluster are aggregated per date and time and the aggregated parking capacity, parking price and occupancy values are taken as representatives for the cluster.
	
	The model training and evaluation is performed in Python via the \textit{scikit-learn} library.
	
	During the training phase, we evaluate different machine learning approaches using decision trees, support vector machines, multilayer perceptrons and boosted trees. As error metric we use \textit{root mean square error (RMSE)} and perform a ten-fold cross-validation. A model will be evaluated on other clusters with parking occupancy data.
	
	\subsection{Build mathematical representations for city areas}
	Using amenity data from OSM and time-spend information we build cluster vectors and density estimation kernels respectively.
	
	To form the cluster vectors, we first divide all amenities into categories $Cat_1, Cat_2, ..., Cat_n$. The criteria for division will be their average visiting duration. For example, a short duration category of up to 30 minutes, a medium duration between 31 and 90 minutes and a large duration of above 90 minutes stay. Each cluster gets represented by an $n$-dimensional vector, whose components correspond to the amenity categories. The magnitude of component $i$ is equal to the number of amenities of category $Cat_i$ that can be found in that particular cluster. Compare \cref{fig:cluster_vector} for a general representation.
	
	\begin{figure}[!ht]
		\centering
		\includestandalone[width=0.7\textwidth]{../graphics/cluster_vector}
		\caption{An example of a cluster vector for three categories.}
		\label{fig:cluster_vector}
	\end{figure}

	A cluster Gaussian is a \textit{kernel density estimation} among amenity probability distributions. In turn, an amenity probability distribution is represented as a \textit{Gaussian kernel}. To construct a cluster Gaussian, we first collect the average visiting duration and standard deviation of the individual amenities. A cluster that contains one amenity $A$ is represented as a Gaussian curve, i.e., normal probability distribution. The curve's center is at the average duration of the amenity $A$ and its standard deviation is the one of the amenity. When $n$ amenities $A$ exist in the cluster, the representation will be an $A$ curve multiplied $n$ times. Multiple amenities, each appearing multiple times, will result in a curve that is the linear combination of the individual representations of the amenities as normal distribution curves. Compare \cref{fig:gaussian} for a visualization of the summing process.
	
	\begin{equation}
	emd(\mathcal{C}_i) = \sum_{j=1}^{|amenities|} K_{ij} \times A_j
	\end{equation}
	
	$$\forall i \in \{1,..|clusters|\} \text{ and } \forall j \in \{1,..|amenities|\}$$
	
	where $A_j$ is an amenity that appears $K_{ij}$ times in the cluster $\mathcal{C}_i$.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.8\textwidth]{../graphics/emd_gaussian_addition3.png}
		\caption{The iterative summing of Gaussians resulting in a cluster Gaussian - the outer graph.}
		\label{fig:gaussian}
	\end{figure}
	
	\subsection{Compute similarity values between city areas}
	The \textit{cosine similarity} between two vectors is defined as the cosine of the angle between the two vectors:
	
	\begin{equation}
	cos(\theta)=\frac{A\cdot B}{{\lVert}A{\rVert}       
		_2{\lVert}B{\rVert}_2}=\frac{\sum_{i=1}^n{A_iB_i}}{\sqrt{\sum_{i=1}^n{A_i^2}}\sqrt{\sum_{i=1}^n{B_i^2}}}
	\end{equation}
	where $A_i$ and $B_i$ are the components of vector A, respective B. 
	
	Unlike the earth mover's distance, the cosine similarity implementation uses the direct mathematical formula by plugging in the magnitudes of the respective vector components.
	
	The \textit{earth mover's distance (emd)} is a measure used in statistics that roughly expresses the difference between position and magnitude of two curves. It is best explained by regarding the curves as the hull of earth piles. For two separate earth piles, emd computes the minimum effort of rearranging a pile so that the shape of the other pile is obtained. Moving P particles over a distance D is equal to the effort $P \times D$. A prerequisite for this operation is that the two piles need to contain the same quantity of earth.
	
	More rigorously, the earth mover's distance is better known in mathematics under the name \textit{Wasserstein Metric}. Given two normal distributions $\mu_1=\mathcal{N}(m_1,C_1)$ and $\mu_2=\mathcal{N}(m_2,C_2)$, where $m_1$ and $m_2 \in \mathbb{R}^{n}$ are their respective expected values and $C_1$ and $C_2 \in \mathbb{R}^{n\times n}$. Then, the 2-Wasserstein distance between $\mu_1$ and $\mu_2$ is:
	
	\begin{equation}
	W_2(\mu_1,\mu_2)^2={\lVert}m_1-m_2{\rVert}^2_2+trace(C_1+C_2-2(C_2^{1/2}C_1C_2^{1/2})^{1/2})
	\end{equation}
	
	In practice, we will not apply the Wasserstein metric directly, but rather resort to some levels of discretization. First off, a number of so-called \textit{bins} is determined. Each bin represents a unit on the $X$ axis, the same on which the visiting duration is expressed. We will take a number of bins equal to the maximum amenity mean and add $3\times$ the largest standard deviation to it, as it is known that within $3\times$ standard deviation on both sides of the mean over 99\% of the Gaussian sum is covered. Moreover, an offset on the $X$-axis equal to $3\times$ the maximum standard deviation is used. This way, we are sure the landscape of summed Gaussians will easily fit into the number of bins.
	
	Notice that emd is applicable only when the sum under both Gaussian curves is equal. Therefore, all cluster Gaussians will get normalized before emd is computed.
	
	\subsection{Computing parking occupancy estimations}
	Once all models $\mathcal{M}$ have been built for the clusters \textit{with} parking data, making estimations on parking occupancy in these areas is straightforward. However, we want to apply these models on the clusters that are missing parking data. We derive the \textit{estimation interval} for cluster $\mathcal{C}_{wout}^j$ based on the model of cluster $\mathcal{C}_{with}^i$ as follows.
	
	For cosine similarity:
	\begin{equation}
	E(\mathcal{C}_{with}^i,\mathcal{C}_{wout}^j) = [\mathcal{M}(\mathcal{C}_{with}^i) - (1 - sim_{ij}), \text{    } \mathcal{M}(\mathcal{C}_{with}^i) + (1 - sim_{ij})]
	\end{equation}
	$$\text{where } sim_{ij} = sim(\mathcal{C}_{with}^i,\mathcal{C}_{wout}^j) \in [0,1]$$
	
	For earth mover's distance:
	\begin{equation}
	E(\mathcal{C}_{with}^i,\mathcal{C}_{wout}^j) = [\mathcal{M}(\mathcal{C}_{with}^i) - emd_{ij},   \text{    }\mathcal{M}(\mathcal{C}_{with}^i) + emd_{ij}]
	\end{equation}
	$$\text{where } emd_{ij} = emd(\mathcal{C}_{with}^i,\mathcal{C}_{wout}^j) \in [0,1]$$
	
	$$\forall i \in \{0,...,|\mathcal{C}_{with}|-1\} \text{ and } \forall j \in \{0,...,|\mathcal{C}_{wout}|-1\}$$.
	
	$X$ is a parking data record containing feature values.
	The result is an \textit{estimation interval} that ``stretches'' the punctual estimation into an interval depending on the similarity value. The lower the similarity value is, the larger the length of the resulting estimation interval will be.
	
	Notice that $X$ should be valid for both $\mathcal{C}_{with}^i$ and $\mathcal{C}_{wout}^j$. 
	The features that are missing in the target cluster $\mathcal{C}_{wout}^j$, i.e., price rate, parking capacity - will be filled in using the respective average values of the whole parking data. 
	
	Furthermore, we define an \textit{estimation intersection interval}, whose purpose is to narrow down the computed estimation interval.
	An estimation intersection interval for the clusters $\mathcal{C}_{with}^i$ and $\mathcal{C}_{wout}^j$ is computed by intersecting the \textit{estimation intervals} that have a better similarity among the clusters with data $\mathcal{C}_{with}^{0}, ..., \mathcal{C}_{with}^{i-1}$ and the same cluster without data $\mathcal{C}_{wout}^j$.
	
	\begin{equation}
	EII(\mathcal{C}_{with}^i,\mathcal{C}_{wout}^j) = \bigcap_{k=0}^{i-1} EI(\mathcal{C}_{with}^k,\mathcal{C}_{wout}^j) \\
	\end{equation}
	
	where
	\begin{equation}
	sim(\mathcal{C}_{with}^k,\mathcal{C}_{wout}^j) < sim(\mathcal{C}_{with}^i,\mathcal{C}_{wout}^j), k \in \{0,..,i - 1\} \text{ for emd}\\
	\end{equation}
	\begin{equation}
	sim(\mathcal{C}_{with}^k,\mathcal{C}_{wout}^j) > sim(\mathcal{C}_{with}^i,\mathcal{C}_{wout}^j), k \in \{0,..,i - 1\} \text{ for cosine} 
	\end{equation}
	
	$$\forall i \in \{0,...,|\mathcal{C}_{with}|-1\} \text{ and } \forall j \in \{0,...,|\mathcal{C}_{wout}|-1\}$$.
	
	\section{Evaluation setup}
	In the following, practical considerations are made in preparation for the evaluation itself. The sources, formats and other relevant specifics for the data used will be described. Included are parking data, OpenStreetMap data and time-spend information.
	
	As parking data, we use the SF\textit{park} project. The San Francisco city project collected extensive data to improve its parking situation in 2011. Besides parking occupancy, the main dataset contain information on parking lot capacity and parking price. Further datasets include traffic, event, weather and gas price information.
	
	As city data, we take OpenStreetMap data from the San Francisco location. The point and polygon layers contain amenity information about the buildings. Furthermore, the polygon layers enables us to compute the area of the buildings, which will be used to complement the parking profiles.
	
	Visiting duration values were extracted from Google Places for the various amenities. The values were collected manually from San Francisco amenities, however we hope that an API will allow this operation to be performed programattically in the near future.
	
	\subsection{The data in detail}
	The SF\textit{park} data are visualized in Figure \ref{fig:before_clustering} using a Leaflet application built as part of this work.
	The actual SF\textit{park} data has some peculiarities.
	While the \textit{occupancy data} is provided with reference to \textit{blocks} as location units, all the other data sets use different location units.
	For the traffic and events data sets, the location units are street names.
	For parking revenue, they are districts.
	In case of weather and fuel price, the location reference is valid for the whole city of San Francisco. 
	Hence, there is a need to align the different data sets before they can be used together.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.8\textwidth]{../graphics/initial_view_before_clusteringV7.png}
		\caption{The blocks accounted in SF\textit{park}. The light blue ones are blocks \textit{without} parking data, the light red ones are \textit{with} parking data.}
		\label{fig:before_clustering}
	\end{figure}
	
	Following the selection of SF\textit{park} data as parking data, the city data is found in the corresponding OpenStreetMap layer for San Francisco.
	The types of the public amenities collected from the POIs are listed in \cref{tab:amenities_list}.
	
	\begin{table}[!ht]
		%\scriptsize
		\tbl{List of all OSM amenities found in the SF\textit{park} blocks.}
		{\begin{tabular}{ | l l l l l | }
				\hline
				arts\_centre & dojo & marketplace & shelter & conference\_centre \\
				bank & embassy & music\_rehearsal\_place & shop & fire\_station \\
				bar & fast\_food & music\_school & spa & fuel \\
				biergarten & grocery & nightclub & stripclub & parking \\
				bureau\_de\_change & gym & pet\_grooming\_shop & studio & place\_of\_worship \\
				cafe & hookah\_lounge & pharmacy & training & social\_centre \\
				clinic & ice\_cream & police & veterinary & swimming\_pool \\
				clothes\_store & karaoke & post\_office & vintage\_and\_modern\_resale & theatre \\
				community\_centre & lan\_gaming\_centre & pub & bus\_station & training \\
				dentist & laundry & restaurant & car\_rental & bicycle\_parking \\
				doctors & library & salon & childcare & car\_wash \\
				brokarage & community\_centre & courthouse & fountain & nursing\_home \\
				recycling & social\_facility & toilets & & \\ 
				\hline
		\end{tabular}}
		\label{tab:amenities_list}
	\end{table}
	
	In case of SF\textit{park}, the blocks are given in latitude and longitude.
	In OpenStreetMap, the geometry is set to EPSG 4326.
	With both systems using the same reference, we can define a \textit{merge distance}.
	This merge distance $d$ is the distance between a parking lot and an amenity, under which we assume that the visitors of the amenity would use the parking lot.
	Further, in the evaluation we assign it distances of 100m, 200m, and 400m.
	
	\subsection{Clustering in practice}
	As established in the approach section, we apply K-Means to cluster the city areas.
	In the evaluation, we will refer to the number of clusters \textit{with} parking data as \textit{the number of clusters}.
	The area without parking data is going to be split into a proportional number of clusters, as the sizes of clusters should be kept roughly equal for both sides.
	It turns out that for SF\textit{park} the proportion is approximately $2.6$, following the division between the total number of blocks from each group.
	We have chosen two numbers of clusters to run the evaluation, namely 8 clusters and 16 clusters.
	The area without parking data will therefore have 20 and 41 clusters, respectively. 
	
	After running the K-Means clustering process, the Leaflet application map reveals the individual clusters by highlighting them on mouse-over.
	The clusters \textit{with} parking data will turn dark red, while the clusters \textit{without} parking data will appear in dark blue (cf. \cref{fig:highlighted_collage}). 
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=\textwidth]{../graphics/highlighted_collage.png}
		\caption{Highlighted cluster \textit{with} parking data on the left side and a cluster \textit{without} parking data on the right side.}
		\label{fig:highlighted_collage}
	\end{figure}
		
	The resulting aggregated blocks are worth taking into account when training the machine learning models, as these will average over pieces of information contained in individual blocks.
	
	Before computing \textit{cluster vectors} and \textit{cluster Gaussians}, we will establish the visiting duration in every amenity.
	For this, we use information gathered from Google Places available via Google Maps\footnote{http://maps.google.com}.
	
	We manually collected information from 470 places in San Francisco, for which a maximum duration of stay was provided (the minimum duration is not always given, as indicated earlier).
	The data was obtained by manually navigating to every business place and writing the duration visit information in a spreadsheet.
	This piece of information is not accessible yet via the Google Places API\footnote{Google Feature Request: \url{https://issuetracker.google.com/issues/35827350}}.
	The results are shown in \cref{tab:amenities_google_places} and the numbers are given in minutes and have been rounded to the nearest integer. We have included only amenities for which at least two stay duration sources were found. 
	
	\begin{table}[!ht]
		\tbl{All amenities listed with their corresponding mean visiting information as collected from Google Places. 
			The \emph{cat} column indicates whether the average visiting time is under half an hour (1), 31 to 90 minutes (2), or more than 1.5 hours (3).
			}
			%\footnotesize
			{\begin{tabular}{ | l | c | c | c || l | c | c | c |}
					\hline		
					\textbf{amenity name} & \textbf{mean} & \textbf{stdev} & \textbf{cat} & \textbf{amenity name} & \textbf{mean} & \textbf{stdev} & \textbf{cat} \\ \hline
					arts\_centre  & 110 & 37 & 3 & laundry  & 78 & 16 & 2 \\ \hline
					bank  & 42 & 65 & 2 &  library  & 83 & 13 & 2 \\ \hline
					bar  & 121 & 38 & 3 &  music\_school  & 120 & 30 & 3 \\ \hline
					cafe  & 76 & 39 & 2 &  nightclub  & 189 & 20 & 3 \\ \hline
					clinic  & 100 & 29 & 3 &  pharmacy  & 25 & 20 & 1 \\ \hline
					clothes\_store  & 41 & 37 & 2 &  post\_office  & 16 & 2 & 1 \\ \hline
					community\_centre  & 119 & 40 & 3 &  pub  & 135 & 21 & 3 \\ \hline
					dentist  & 104 & 35 & 3 &  restaurant  & 135 & 32 & 3 \\ \hline
					doctors  & 60 & 42 & 2 &  salon  & 141 & 53 & 3 \\ \hline
					embassy  & 75 & 24 & 2 &  shelter  & 90 & 0 & 2 \\ \hline
					fast\_food  & 31 & 15 & 2 &  shop  & 43 & 21 & 2 \\ \hline
					grocery  & 20 & 10 & 1 &  spa  & 161 & 54 & 3 \\ \hline
					gym  & 100 & 22 & 3 &  stripclub  & 140 & 46 & 3 \\ \hline
					hookah\_lounge  & 130 & 17 & 3 &  studio  & 60 & 0 & 2 \\ \hline
					ice\_cream  & 23 & 7 & 1 &  veterinary  & 67 & 29 & 2 \\ \hline
					karaoke  & 188 & 15 & 3 & {\scriptsize vintage\_modern\_resale}  & 38 & 32 & 2 \\ \hline
			\end{tabular}}
			\label{tab:amenities_google_places}
			\begin{tabnote}
				Duration and standard deviation are expressed in minutes. The assigned category for cluster vectors is included.
			\end{tabnote}
	\end{table}
						
	Alongside this information, we need the amenity categories in order to derive the \textit{cluster vectors}.
	The categories are based on the visiting duration mean and are split them in three categories: 
	\begin{romanlist}
		\item under half an hour,
		\item 31 to 90 minutes and
		\item more than 1.5 hours. 
	\end{romanlist}
	The assigned partitions for every amenity are shown in \cref{tab:amenities_google_places}. 
	The computation of \textit{cluster Gaussians} relies on both the mean and standard deviation of the amenity visiting duration.

	\subsection{Amenity Area as Similarity Basis}
	\label{experimental_setup:amenity_area}
	We considered \textit{visiting duration} as the basis for creating the urban profile. However, there are other factors that affect the parking demand towards an amenity.
	Obviously one of them is the amount of people visiting the amenity.
	As we do not have data about this aspect, we use the area of the amenity as a proxy, assuming that larger places would have more visitors.
	OpenStreetMap provides a polygon layer for a certain geographic bounding box, which contains information across all the surfaces in that region. See \cref{fig:amenity_polygons} for a visualization of the polygons and their areas in OSM. 
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=\textwidth]{../graphics/amenity_polygons4.png}
		\caption{OSM screenshot emphasizing polygons as buildings and the amenities that are housed by them}
		\label{fig:amenity_polygons}
	\end{figure}
	
	To extract this information we investigated two options.
	\begin{romanlist}
		\item \textbf{Polygon containing POI}. 
		Matching the amenities' POIs with the containing OSM polygon and then computing the polygon areas per amenity was the option tried first. 
		This has several drawbacks. 
		The relation POI $\colon$ polygon is in practice by no means 1 $\colon$ 1.
		Many cases arose where multiple POIs were contained by the same polygon, in which situation the area was split between them; a POI might also be on the edge of several polygons, in which case we have to either (arbitrarily) assign it to the first polygon or to all. 
		The deciding factor against this approach was, however, the fact that the \textit{coefficient of variation}, i.e., the ratio between the standard deviation and the mean of the sample, is larger than $1$, i.e., $2.1$ to be precise.  
		
		\item \textbf{Amenity attribute in polygon layer}. The other option was to use the \textit{amenity} attribute from the polygon layer of the region. 
		We could avoid the cumbersome matching by leveraging solely the polygon layer and calculating the amenity area mean and its standard deviation. The results are listed in \cref{tab:amenity_area_values}. 
		On top of that, the coefficient of variation is $0.9$ in this case, significantly lower than before. 
		
		Note that we have reduced the values in the table by a factor of 20, as it turned out that the actual mean and standard deviation were large enough to make the emd Gaussian computation extremely slow. As the standard deviation is linear with regard to the mean, both mean and standard deviation values were reduced conveniently. 
		
	\end{romanlist}
	
	\begin{table}[!ht]
		%\footnotesize
		\tbl{Amenity area values gathered and averaged from OMS polygon layer for the SF\textit{park} region}
		{\begin{tabular}{ | l | c | c | c || l | c | c | c |}
				\hline		
				\textbf{amenity name} & \textbf{mean} & \textbf{stdev} & \textbf{cat} & \textbf{amenity name} & \textbf{mean} & \textbf{stdev} & \textbf{cat} \\ \hline
				arts\_centre & 68 & 60 & 2 & bank & 39 & 20 & 2 \\ \hline
				bar & 19 & 8 & 1 & bicycle\_parking & 8 & 7 & 1 \\ \hline
				biergarten & 11 & 12 & 1 & brokerage & 39 & 9 & 2 \\ \hline
				bus\_station & 588 & 737 & 3 & cafe & 17 & 10 & 1 \\ \hline
				car\_rental & 70 & 43 & 2 & car\_wash & 43 & 48 & 2 \\ \hline
				childcare & 101 & 130 & 3 & cinema & 75 & 43 & 2 \\ \hline
				clinic & 61 & 32 & 2 & community\_centre & 52 & 74 & 2 \\ \hline
				conference\_centre & 401 & 519 & 3 & courthouse & 459 & 201 & 3 \\ \hline
				dentist & 17 & 12 & 1 & doctors & 324 & 568 & 3 \\ \hline
				embassy & 68 & 38 & 2 & fast\_food & 25 & 24 & 1 \\ \hline
				fire\_station & 52 & 27 & 2 & fountain & 24 & 22 & 1 \\ \hline
				fuel & 25 & 27 & 1 & library & 102 & 124 & 3 \\ \hline
				marketplace & 325 & 228 & 3 & music\_rehearsal\_place & 33 & 15 & 1 \\ \hline
				nightclub & 32 & 9 & 1 & nursing\_home & 97 & 47 & 2 \\ \hline
				parking & 182 & 309 & 3 & pharmacy & 65 & 38 & 2 \\ \hline
				place\_of\_worship & 60 & 62 & 2 & police & 137 & 124 & 3 \\ \hline
				post\_office & 39 & 11 & 2 & pub & 25 & 25 & 1 \\ \hline
				public\_building & 280 & 236 & 3 & recycling & 28 & 20 & 1 \\ \hline
				restaurant & 22 & 16 & 1 & school & 740 & 1280 & 3 \\ \hline
				social\_centre & 30 & 21 & 1 & social\_facility & 356 & 801 & 3 \\ \hline
				stripclub & 50 & 10 & 2 & studio & 268 & 307 & 3 \\ \hline
				swimming\_pool & 16 & 9 & 1 & swingerclub & 27 & 4 & 1 \\ \hline
				theatre & 174 & 191 & 3 & toilets & 7 & 5 & 1 \\ \hline
				training & 72 & 94 & 2 & veterinary & 21 & 7 & 1 \\ \hline
		\end{tabular}}
		\label{tab:amenity_area_values}
		\begin{tabnote}
			The mean and standard deviation values were reduced by a 20x factor. The categories for cosine vectors are 0 - 35, 36 - 100, 100+.
		\end{tabnote}
	\end{table}
	
	\subsection{Preprocessing training data}
	As training data, the SF\textit{park} \textit{occupancy data} is used with \textit{street blocks} as location unit. 
	It turns out that training on the additional SF\textit{park} data, i.e., traffic and events, encounters some problems. 
	
	The \textit{traffic data} do not share the same location unit with the parking occupancy's street block. Even when aggregating traffic data on the district level, which is available for the occupancy data as well, it did not prove an additional value to the training. 
	
	For the SF\textit{park} \textit{events data} we encounter the same problem as for the traffic data: the location unit does not match the block. In fact, the events are marked for streets, whose association to blocks is not determinable. 
	
	\textit{Parking revenue data} is provided for districts, which again are too general to make a difference in training.
	
	Finally, \textit{weather data} and \textit{fuel data} are given per city, hence making even less an impact to improve the model.
	
	As indicated in the approach section \cmmnt{\cref{realization:machine_learning_models}}, the training data is aggregated across all blocks so that it becomes comparable to other clusters and can be used in training and testing models. The averaging is performed per timestamp, i.e., if multiple blocks have an occupancy record for the same time and block, the occupancy rate will be averaged for both of these. Features such as \textit{price} and \textit{parking capacity per block} are averaged as well. See \cref{tab:aggregating_datapoints} for an example of this process. This means that the original collection of data records shrinks, which should decrease the training time. In \cref{tab:models_training_points}, the expansion/shrinking rate is shown for various number of clusters.
	
	\begin{table}
		%\footnotesize
		\tbl{Example of aggregating datapoints}
		{\begin{tabular}{ c }
				\begin{tabular}{ | c | c | c | c | c |}
					\hline
					\textbf{block id} & \textbf{timestamp} & \textbf{price rate} & \textbf{total spots} & \textbf{occupied} \\ \hline
					902   & {2011-04-02 7:00:00} & 0 & 46 & 58 \\ \hline
					32800 & {2011-04-02 7:00:00} & 0 & 32 & 2 \\ \hline
					33005 & {2011-04-02 7:00:00} & 3 & 36 & 12 \\ \hline
					902   & {2011-04-02 8:00:00} & 2 & 46 & 54 \\ \hline
					32800 & {2011-04-02 8:00:00} & 4 & 32 & 5 \\ \hline
					33005 & {2011-04-02 8:00:00} & 3 & 36 & 22 \\ \hline
				\end{tabular} \\
				%\vskip 0.2cm
				%\\
				\colrule
				%\vskip 0.2cm
				%\\
				%\hskip 1.8cm
				\begin{tabular}{ | c | c | c | c |}
					\hline
					\textbf{timestamp} & \textbf{price rate} & \textbf{total spots} & \textbf{occupied} \\ \hline
					{2011-04-02 7:00:00} & \textbf{1} & \textbf{38} & \textbf{24} \\ \hline
					{2011-04-02 8:00:00} & \textbf{3} & \textbf{38} & \textbf{27} \\ \hline
				\end{tabular}
		\end{tabular}}
		\begin{tabnote}
			In the subtable above, three distinct blocks belonging to a cluster are transformed into two entries by averaging \textit{price rate}, \textit{total spots} and \textit{occupied} attributes for the two distinct timestamps (subtable below)
		\end{tabnote}
		\label{tab:aggregating_datapoints}
	\end{table}
	
	\begin{table}[!ht]
		%\setlength\extrarowheight{5pt}
		%\centering
		%\footnotesize
		\tbl{Number of datapoints aggregated per timestamp vs. all datapoints alongside the shrinking rate for 8, 16 and 32 clusters}
		{\begin{tabular}{ | c | c | c | c | } %| c | S[table-format=5.0] | S[table-format=6] | S[table-format=2.1] |
				\hline
				{cluster size} & {aggregated datapoints} & {all datapoints} & {shrinking/expansion rate} \\ \hline
				8 &	9741 & 128525 &	12.3 \\ \hline
				16 & 8409 &	73332 &	8.3 \\ \hline
				32 & 6257 &	29355 &	4.6 \\ \hline
		\end{tabular}}
		\begin{tabnote}
			Values have been averaged across clusters
		\end{tabnote}
		%\captionsetup{justification=centering}
		\label{tab:models_training_points}
	\end{table}
		
	%\subsection{Model Evaluation for Clusters \textit{with} Parking Data}
	%\label{evaluation:training_testing_errors}
	In \cref{fig:cwith} a screenshot of the web application showing a sample of the results is illustrated. \Cref{fig:cwith_table} displays the presented table in more detail.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.8\textwidth]{../graphics/cwith_source_dt_cosine.png}
		\caption{Selected cluster \textit{with} parking data and the pop-up table in the Leaflet application.}
		\label{fig:cwith}
	\end{figure}
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.8\textwidth]{../graphics/cwith_source_dt_cosine_table.png}
		\caption{The pop-up table for the Leaflet application view of \cref{fig:cwith}.}
		\label{fig:cwith_table}
	\end{figure}
	
	\section{Evaluation}
	We evaluate various pieces of the system that has been presented. Firstly, in \cmmnt{\cref{evaluation:best_model}}, we  establish the machine learning method that achieves best results on average across clusters. \cmmnt{\Cref{evaluation:similarity_vs_estimation_sec}} is the heart of the evaluation, in which we compare the cluster models' test error with the independently-computed similarity values between clusters. More specifically, a \textit{source} cluster's model will tested on a \textit{target} cluster and the error is correlated to the similarity between the \textit{source} and the \textit{target} clusters. Both \textit{cosine} and \textit{emd} functions will be used. The correlations will be expressed as Pearson- and Spearman's rank coefficients. Afterwards, in \cmmnt{\cref{evaluation:estimations_cwout}}, we take a look at the results of applying the models to clusters \textit{without parking data} and showcase the web application. \cmmnt{\Cref{evaluation:entire_datapoints}} looks at the model test errors and correlation results by skipping the aggregating step, i.e., instead of averaging the datapoints over timestamp per cluster, we build the cluster models using the entire occupancy data directly. \cmmnt{\Cref{evaluation:amenity_area}} follows up on \cmmnt{\cref{experimental_setup:amenity_area}} and uses the \textit{amenity area} as the basis for the similarity functions in calculating the correlations between model test errors and similarity values. Finally, in \cmmnt{\cref{evaluation:machine_learning_better}} we question whether the similarity function approach is the most efficient and transfer its purpose to the machine learning phrase. The model will receive absolute \textit{cosine} and \textit{emd Gaussian} values as additional features and its model test error and correlation values will be compared to the ones from the original approach.
	
	\subsection{Best model method}
	Models were trained using four methods: decision trees, support vector machines, multilayer perceptrons, and gradient boosted trees. \Cref{evaluation:best_model_method} shows the distribution of best machine learning methods in case of 8 and 16 clusters.
	The values were obtained by summing up the number of times a method produced the smallest estimation error, i.e., RMSE, among the four methods for all combinations of clusters with parking data $(\mathcal{C}_{source}, \mathcal{C}_{target})$.
	Extreme gradient boosting claims the first spot in both cases.
	In further experiments in the evaluation we shall only report on models with extreme gradient boosting.
	
	\begin{table}[!ht]
		%\centering
		%\small
		%\setlength\extrarowheight{5pt}
		\tbl{The proportion of best models among decision trees, support vector machine, multilayer perceptron and gradient boosting  measured as RMSE when applied on all pairs of clusters}
		{\begin{tabular}{ | c | c | c | c | c |}
				\hline
				& \textbf{dt} & \textbf{svm} & \textbf{mlp} & \textbf{xgb} \\ \hline
				\textbf{8 clusters} & 24.6\% & 17.5\% & 12.3\% & \textbf{45.6\%} \\ \hline
				\textbf{16 clusters} & 14.6\% & 13.8\% & 13.8\% & \textbf{57.9\%} \\ \hline
		\end{tabular}}
		\label{evaluation:best_model_method}
	\end{table}
	
	\subsection{Similarity Values vs. Estimation Errors}
	The central goal of this work is estimate the occupancy of clusters where no parking data is available by using model predictions and pair-wise cluster similarity values. 
	
	%If the model prediction accuracy can be measured on clusters with parking data by means of their test errors, we still need to show that the cluster similarity values indeed express a similarity of occupancy.
	 
	We shall use clusters with parking data as the target application of the prediction models. 
	%and use the model predictions as occupancy references
	The higher the absolute correlation between the cluster similarity values and the model test errors, the better the accuracy of the cluster similarity. 
	
	Concretely, for every pair of clusters ($\mathcal{C}_{source}, \mathcal{C}_{target}$), a model $\mathcal{M}_{\mathcal{C}_{source}}$ is trained on $\mathcal{C}_{source}$ and its test error ($\mathcal{M}_{\mathcal{C}_{source}}(\mathcal{C}_{target})$) shall be correlated with the cosine and emd similarity values between $\mathcal{C}_{source}$ and $\mathcal{C}_{target}$. Here we use two correlation coefficients: the Pearson correlation coefficient and Spearman's rank correlation coefficient, which results in four correlation measures: 
	\begin{romanlist}
		\item	cosine (Pearson) correlation
		\item 	cosine (Spearman's) rank correlation
		\item 	emd (Pearson) correlation
		\item 	emd (Spearman's) rank correlation. 
	\end{romanlist}
	The evaluation was performed for configurations of 8 and respectively 16 clusters. Additionally, we varied the \textit{merge distance} to see how the correlation behaves. The similarity values are hence calculated for 100m, 200m, and 400m merge distance respectively. In \cref{tab:similarity_vs_estimation} the final results are shown. For each correlation measure (\textit{cosine}, \textit{rank cosine}, \textit{emd}, \textit{rank emd}) the value is averaged across all clusters. Due to their mathematical meaning explained in \cmmnt{\cref{realization:similarity_functions}} the Approach section, the cosine similarity values below zero express a positive correlation, whereas emd values above zero express a positive correlation. The models were trained with gradient boosted trees. 
	
	We notice that the cosine similarity achieves better results than emd for the same testing configuration, peaking for 8 clusters and 100m merge distance. Its average Pearson coefficient is $-0.55$, while the mean Spearman rank coefficient is $-0.49$. emd positively correlates the most for the same testing configuration (8 clusters, 100m), when the average Pearson coefficient is at $0.28$ and Spearman's rank coefficient equals $0.23$. There is a clear descending trend in correlations, as the merge distance increases.  Also, the results for 8 clusters are superior to the ones when the city is split in 16 clusters. 
	
	Further in the evaluation runs we shall fix the merge distance to 100m.
	
	\begin{table}[!ht]
		%\centering
		%\small
		%\setlength\extrarowheight{5pt}
		%\resizebox{\textwidth}{!}{
		\tbl{Correlations between similarity values and model estimations errors for pairs of clusters \textit{with} parking data ($\mathcal{C}_{source}, \mathcal{C}_{target})$.}
		{\begin{tabular}{ | c | c | c | c | c | c | c | c | c |}
				\hline
				{} & \multicolumn{4}{c|}{8 clusters} \\ \hline
				{merge distance} & cosine & rank\_cosine & emd & rank\_emd \\ \hline
				100m & \textbf{-0.55} & \textbf{-0.49} & \textbf{0.28} & 0.23 \\ \hline
				%100m & \textbf{100\%} & \textbf{100\%} & \textbf{87.5\%} & \textbf{75\%} \\ \hline
				200m & -0.34 & -0.30 & 0.26	& 0.23 \\ \hline
				%200m & 75\% & 75\% & 75\% & 75\% \\ \hline
				400m & -0.23 & -0.08 & 0.25 & \textbf{0.27} \\ \hline
				%400m & 62.5\% & 50\% & 75\% & 75\% \\ \hline
		\end{tabular}}
		\vspace{3em}
		{\begin{tabular}{ | c | c | c | c | c | }
				\hline
				{} & \multicolumn{4}{c|}{16 clusters} \\ \hline
				{merge distance} & cosine & rank\_cosine & emd & rank\_emd \\ \hline
				100m & \textbf{-0.20} & \textbf{-0.17} & \textbf{0.10} & \textbf{0.11} \\ \hline
				%100m & 75\% & 75\% & 68.75\% & 62.5\% \\ \hline
				200m & -0.13 & -0.11 & 0.02 & 0.02 \\ \hline
				%200m & 75\% & 75\% & 56.3\% & 56.3\% \\ \hline
				400m & -0.17 & -0.17 & 0.08 & 0.11 \\ \hline
				%400m & 68.8\% & 68.8\% & 62.5\% & 62.5\% \\ \hline
		\end{tabular}}
		\label{tab:similarity_vs_estimation}
		\begin{tabnote}
			For cosine similarity the values show a negative correlation tendency, while for the correlation based on emd similarity expresses a positive correlation tendency. The correlations are measured using Pearson coefficient and Spearman's rank coefficient.
		\end{tabnote}
	\end{table}
	
	\subsection{Estimations for clusters without Parking Data}
	We apply the models trained on SF\textit{park} data on clusters \textit{without} parking data.
	The testing data records are composed of values equal to the averages of the respective data types in all clusters \textit{with} parking data.
	This is the case for \textit{parking price} and \textit{parking capacity}.
	One piece of data that still needs to be provided so that the estimation is computed is the date and time.
	For convenience, we choose the next day.
	% as the point when the user starts the model training. 
	An example of the estimation is visualized \cref{fig:cwout_table}.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.8\textwidth]{../graphics/cwout_cosine_table.png}
		\caption{The pop-up table of a cluster without data.
			Notice the drop-down list from which the time can be selected. The similarity values are rounded off. }
		\label{fig:cwout_table}
	\end{figure}
	
	\subsection{Models built on all occupancy datapoints}
	Up to now, the evaluation involved models trained and tested- on aggregated datapoints, as explained in \cmmnt{\cref{experimental_setup:aggregating_training_data}} the Experimental Setup section. We ask ourselves, however, whether a model trained on all datapoints performs better when tested on an aggregated cluster. Or whether an aggregate model delivers better results on an all-datapoints cluster than a model trained on all datapoints? Here we experiment these combinations by training models on both all- and aggregate datapoints and apply them on both types of aggregation forms. 
	
	See \cref{tab:train_test_errors} for an overview of test errors for 8 and 16 clusters. One observes that the errors from models applied on aggregated datapoints are about 5 unit points (=5\%) smaller than the errors from models applied on all datapoints. This is naturally accounted for by the smaller spread of occupancy values that the aggregation brings with itself. As far as source models are concerned, there is no significant difference between the aggregate and all datapoint models, i.e. the margin is under 1\%. Regarding the number of clusters, the values for 8-cluster models are slightly better than 16-cluster models for aggregated datapoints, but lose for when the testing bed is all datapoints.
	
	In \cref{tab:correlation_values} the resulting correlations of testing errors with cosine- and emd similarity values are listed for models of 8 and 16 cluster configurations. It follows that the cosine- and rank cosine correlation values are on average closer to -1 in the 8-clusters case. The same applies for emd- and rank emd correlation values, which are closer to 1 in this case. The correlation values for 16-clusters are obviously weaker than for 8-clusters. In both sections, the aggregate datapoints target is superior to the all datapoints target.
	
	\begin{table}[!ht]
		%\centering
		%\small
		\tbl{Test error for ML models build alternatively with all- and aggregated datapoints}
		%\setlength\extrarowheight{5pt}
		%\resizebox{\textwidth}{!}{
		{\begin{tabular}{ | c | c | c | c | c | c | }
				\hline
				{cluster size} & {datapoints source} & {datapoints target} & {test error} \\ \hline
				8	&	aggregate 	&	aggregate 	& 	\textbf{20.19} 	\\ \hline
				8	&	all 		&	aggregate 	& 	21.37	\\ \hline \hline
				8	&	aggregate 	&	all 		& 	\textbf{26.25}	\\ \hline
				8	&	all			& 	all 		&	26.68	\\ \hline \hline
				16	&	aggregate	& 	aggregate 	&	\textbf{20.47}	\\ \hline
				16	&	all			& 	aggregate 	& 	21.32	\\ \hline \hline
				16	&	aggregate	& 	all 		& 	\textbf{25.97}	\\ \hline
				16	&	all			&	all 		&	26.52	\\ \hline \hline
		\end{tabular}}
		\label{tab:train_test_errors}
		\begin{tabnote}
			Test errors for the same number of clusters can be accurately compared when the \textit{datapoints target} is the same. All models above were build using extreme gradient boosting.
		\end{tabnote}
	\end{table}
			
	\begin{table}[!ht]
		%\centering
		%\small
		%\setlength\extrarowheight{5pt}
		%\resizebox{\textwidth}{!}{
		\tbl{Resulting correlation values for ML models built using all- and aggregated datapoints}
		{\begin{tabular}{ | c | c | c | c | c | c | c | }
				\hline
				{cluster size} & {datapoints source} & {datapoints target} & cosine & rank\_cosine & emd & rank\_emd \\ \hline
				8	&	aggregate 	&	aggregate 	& 	\textbf{-0.53}	&	\textbf{-0.52}	&	0.30	&	0.17 	\\ \hline
				8	&	all 		&	aggregate 	& 	-0.53	&	-0.43	&	\textbf{0.37}	&	\textbf{0.27}	\\ \hline \hline
				8	&	aggregate 	&	all 		& 	-0.35	&	-0.36	&	0.20	&	0.14	\\ \hline
				8	&	all			& 	all 		&	\textbf{-0.41}	&	\textbf{-0.43}	&	\textbf{0.34}	&	\textbf{0.25}	\\ \hline \hline
				16	&	aggregate	& 	aggregate 	&	-0.16	&	-0.11	&	0.10	&	0.05	\\ \hline
				16	&	all			& 	aggregate 	& 	\textbf{-0.18}	&	\textbf{-0.17}	&	\textbf{0.22}	&	\textbf{0.17}	\\ \hline \hline
				16	&	aggregate	& 	all 		& 	-0.09	&	-0.06	&	0.08	&	0.00	\\ \hline
				16	&	all			&	all 		&	\textbf{-0.10}	&	\textbf{-0.11}	&	\textbf{0.17}	&	\textbf{0.08}	\\ \hline \hline
		\end{tabular}}
		\label{tab:correlation_values}
		\begin{tabnote}
			All models above were build using extreme gradient boosting.
		\end{tabnote}
	\end{table}
	
	\subsection{Amenity Area as similarity Basis}
	Following up on \cmmnt{\cref{experimental_setup:amenity_area}} the Experiment Setup section, we take \textit{amenity area} as the basis for the cosine- and emd similarity. The correlations values are listed in \cref{tab:correlation_amenity_area} for 8- and 16 cluster configurations. The superiority of the correlation values using \textit{visiting duration} is observed both for cosine- and rank cosine correlations, which are closer to -1, and for the emd- and rank emd correlations, which are nearer to 1. All correlation values for 16 clusters are, however, relatively weak in absolute measures.
	
	\begin{table}[!ht]
		%\centering
		%\small
		%\setlength\extrarowheight{5pt}
		%\resizebox{\textwidth}{!}{
		\tbl{The correlation results computed using similarity values based on \textit{amenity area}. }
		{\begin{tabular}{ | c | c | c | c | c | c | c | }
				\hline
				{cluster size} 	& {amenity type} 	& datapoints (train/test) 	& cosine 	& rank\_cosine & emd & rank\_emd \\ \hline
				8 				& {visiting duration} 		& agg/agg 		& \textbf{-0.53}	& \textbf{-0.52}		&	\textbf{0.30}	&	\textbf{0.17} \\ \hline
				8 				& area 				& agg/agg 		& 0.05	&	0.11	&	0.14	&	0.22 \\ \hline \hline
				16 				& {visiting duration} 		& agg/agg 		& -0.16	&	-0.11	&	0.10	&	0.05 \\ \hline
				16 				& area 				& agg/agg 		& -0.09	&	-0.03	&	0.09	&	0.07 \\ \hline
		\end{tabular}}
		\begin{tabnote}
			All models above were build using extreme gradient boosting, trained and tested on aggregated datapoints.  
		\end{tabnote}
		\label{tab:correlation_amenity_area}
	\end{table}
	
	\subsection{Extended Prediction Models}
	An alternative to building urban measures and similarity functions is to let the machine learning model find out the similarities by itself. 
	One can choose to add the city data as further training information for clusters. The purpose is to produce better predictions by leveraging unknown patterns in the city data. Hence, features representing the \textit{cosine} and \textit{emd} functions are added to the model, as follows: 
	\begin{romanlist}
		\item k features corresponding to the k categories the amenities are split in, i.e. the magnitudes of the vectors for each category;
		\item a feature corresponding to the emd Gaussian value for that cluster, loosely interpreted as the "total accumulated visiting duration" for that cluster, in case of \textit{visiting duration}, or the "total accumulated area" among all amenities in that cluster, for the amenity area; mathematically it is expressed in Equation \cref{eq:emd_gaussian_magnitude}
		\begin{equation}
		feature(emd)=x \cdot f(x)
		\label{eq:emd_gaussian_magnitude}
		\end{equation}
		{\centering
			where $x$ = 
			$
			\begin{dcases}
			\text{duration in minutes}, & \text{if similarity is \textit{amenity visiting duration}} \\
			\text{area in square meters}, & \text{if similarity is \textit{amenity area}}.
			\end{dcases}
			$ \\
			and $f$ is the constructed emd Gaussian function that equals the number of amenities in the cluster for any value x.}
	\end{romanlist}
	
	We subsequently build \textit{extended machine learning models} that additionally contain the features above. Since these features are identical for all datapoints in a cluster, the model needs to be trained on multiple clusters. Therefore, models on $n-1$ out of $n$ clusters will be build and tested on the remaining cluster. These \textit{all-but-one} or \textit{total models} will be constructed for all $n$ combinations of $n-1$ clusters and their averaged test errors will compared to total models that contain the normal features. The resulting test errors are shown in \cref{tab:extended_models_comparison} for various number of clusters. 
	
	In three out of four cases, the addition of the two features did not help with finding better parking occupancy values for the test clusters, at least using the gradient boost model we used in our experiments. For the 16-cluster case, the total model returns a superior result. 
	
	exception is when the simple total models achieve a better prediction performance. 
	
	\begin{table}[!ht]
		%\footnotesize
		\tbl{Total models extended with cosine and emd features compared to total models with the previous feature set}
		{\begin{tabular}{ | c | c | c | c | }
				\hline		
				\textbf{cluster size}  & \textbf{model} & \textbf{test error average} \\ \hline
				4 & {xgb} & \textbf{14.92} \\ \hline
				4 & {xgb total} & 16.18 \\ \hline \hline
				8 & {xgb} & \textbf{18.12} \\ \hline
				8 & {xgb total} & 19.58 \\ \hline \hline
				%8 & {dt} & \textbf{18.85} & \textbf{18.52} \\ \hline
				%8 & {dt extended} & 19.84 & 19.46 \\ \hline \hline
				%8 & {svm} & TBD & TBD \\ \hline
				%8 & {svm extended} & 20.15 & 23.62 \\ \hline
				%8 & {mlp} & TBD & TBD \\ \hline
				%8 & {mlp extended} & 54.62 & 203.22 \\ \hline
				16 & {xgb} & 18.19 \\ \hline
				16 & {xgb total} & \textbf{18.09} \\ \hline \hline
				%16 & {dt} & 19.14 & 18.83 \\ \hline
				%16 & {dt extended} & \textbf{18.77} & \textbf{17.81} \\ \hline
				%16 & {svm} & TBD & TBD \\ \hline
				%16 & {svm extended} & 22.52 & 23.49 \\ \hline
				%16 & {mlp} & TBD & TBD \\ \hline
				%16 & {mlp extended} & 22.53	& 19.36 \\ \hline
				32 & {xgb} & \textbf{18.65} \\ \hline
				32 & {xgb total} & 20.38 \\ \hline
		\end{tabular}}
		\label{tab:extended_models_comparison}
		%	\begin{tabnote}
		%	\end{tabnote}
	\end{table}

\end{document}